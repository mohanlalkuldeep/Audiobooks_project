{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data from an Audiobook app in .csv format. Logically, it relates only to the audio versions of books. each customer in the database has made a purchase at least once, thats why customer is in database. we want to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\n",
    "\n",
    "The main idea is that if a customer has a low probability of coming back. there is no reason to spend any money on advertising to customer. if we can focus our efforts only on customers that are likely to convert again. we can make great savings. moreover, this model can identify the most important metrics for a customer to come back again. identifying new customer creates value and growth opportunities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt(r\"C:\\Users\\nnhp\\Downloads\\Audiobooks_data (1).csv\", delimiter= \",\")# download the dataset/file from the data folder, pass the path of that file\n",
    "all_unscaled_inputs = raw_data[:,1:-1]\n",
    "all_targets = raw_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(all_targets))\n",
    "counter_Zero_targets= 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range(all_targets.shape[0]):\n",
    "    if all_targets[i] ==0:\n",
    "        counter_Zero_targets +=1\n",
    "        if counter_Zero_targets > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "priors_unscaled_inputs_equal = np.delete(all_unscaled_inputs, indices_to_remove, axis=0)\n",
    "priors_targets_equals = np.delete (all_targets, indices_to_remove, axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_scaled = preprocessing.scale(priors_unscaled_inputs_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_shuffled = np.arange(inputs_scaled.shape[0])\n",
    "np.random.shuffle(indices_shuffled)\n",
    "\n",
    "inputs_shuffled= inputs_scaled[indices_shuffled]\n",
    "targets_shuffled= priors_targets_equals[indices_shuffled]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the datasets into train, validation, and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1805.0 3579 0.5043308186644314\n",
      "234.0 447 0.5234899328859061\n",
      "198.0 448 0.4419642857142857\n"
     ]
    }
   ],
   "source": [
    "count_sample = inputs_shuffled.shape[0]\n",
    "train_count_sample = int(0.8*count_sample)\n",
    "validation_count_sample = int(0.1*count_sample)\n",
    "test_count_sample = count_sample - train_count_sample - validation_count_sample\n",
    "\n",
    "train_inputs = inputs_shuffled[:train_count_sample]\n",
    "train_targets = targets_shuffled[:train_count_sample]\n",
    "\n",
    "validation_inputs= inputs_shuffled[train_count_sample:train_count_sample+validation_count_sample]\n",
    "validation_targets= targets_shuffled[train_count_sample:train_count_sample+validation_count_sample]\n",
    "\n",
    "test_inputs = inputs_shuffled[train_count_sample+validation_count_sample:]\n",
    "test_targets = targets_shuffled[train_count_sample+validation_count_sample:]\n",
    "\n",
    "print(np.sum(train_targets), train_count_sample, np.sum(train_targets)/train_count_sample)\n",
    "print(np.sum(validation_targets), validation_count_sample, np.sum(validation_targets)/validation_count_sample)\n",
    "print(np.sum(test_targets), test_count_sample, np.sum(test_targets)/test_count_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_train_data', final_inputs = train_inputs, final_targets= train_targets)\n",
    "np.savez('Audiobooks_validation_data', final_inputs = validation_inputs, final_targets= validation_targets)\n",
    "np.savez('Audiobooks_test_data', final_inputs = test_inputs, final_targets= test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz= np.load('Audiobooks_train_data.npz')\n",
    "\n",
    "train_x1_inputs = npz['final_inputs'].astype(np.float)\n",
    "train_y1_targets = npz['final_targets'].astype(np.int)\n",
    "\n",
    "npz= np.load('Audiobooks_validation_data.npz')\n",
    "\n",
    "validation_x2_inputs= npz['final_inputs'].astype(np.float)\n",
    "validation_y2_targets= npz['final_targets'].astype(np.int)\n",
    "\n",
    "npz= np.load('Audiobooks_test_data.npz')\n",
    "\n",
    "test_x2_inputs= npz['final_inputs'].astype(np.float)\n",
    "test_y2_targets= npz['final_targets'].astype(np.int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline, Optimizers, loss, early stopping and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 10s - loss: 0.6130 - accuracy: 0.6558 - val_loss: 0.5373 - val_accuracy: 0.7069\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.4699 - accuracy: 0.7622 - val_loss: 0.4599 - val_accuracy: 0.7606\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.4128 - accuracy: 0.7837 - val_loss: 0.4212 - val_accuracy: 0.7808\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.3867 - accuracy: 0.7907 - val_loss: 0.4018 - val_accuracy: 0.8210\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.3737 - accuracy: 0.7958 - val_loss: 0.3870 - val_accuracy: 0.7942\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.3605 - accuracy: 0.8044 - val_loss: 0.3805 - val_accuracy: 0.7987\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.3523 - accuracy: 0.8100 - val_loss: 0.3710 - val_accuracy: 0.8031\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.3484 - accuracy: 0.8092 - val_loss: 0.3644 - val_accuracy: 0.7987\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.3462 - accuracy: 0.8094 - val_loss: 0.3628 - val_accuracy: 0.8031\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.3405 - accuracy: 0.8092 - val_loss: 0.3646 - val_accuracy: 0.8031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1436449bc40>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_size = 10\n",
    "outputs_size = 2\n",
    "hidden_layers_size= 50\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layers_size, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(hidden_layers_size, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(outputs_size, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss= tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "             optimizer= tf.keras.optimizers.Adam(),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping()\n",
    "\n",
    "model.fit(train_x1_inputs,\n",
    "         train_y1_targets,\n",
    "         batch_size= batch_size,\n",
    "         epochs= max_epochs,\n",
    "         callbacks = [early_stopping],\n",
    "         validation_data= (validation_x2_inputs, validation_y2_targets),\n",
    "         verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 6ms/step - loss: 0.3601 - accuracy: 0.8214\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss:0.36, Test accuracy:82.14%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest loss:{0:.2f}, Test accuracy:{1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
